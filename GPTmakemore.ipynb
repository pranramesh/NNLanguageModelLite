{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Architecture"
      ],
      "metadata": {
        "id": "OWowNgMJX3vL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now fully arrived to exploring the transformer architecture to explore the fully idealized version of a character level language model. "
      ],
      "metadata": {
        "id": "kmfkDJu7YBfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bSpLAw5Xj9y",
        "outputId": "dc3b2945-8434-4904-8d4d-b3a98b14b864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-30 18:28:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-30 18:28:28 (61.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "ZvLw-lEbXsC4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmOkQ2SbXyQf",
        "outputId": "1d79ef1f-ee32-4b04-ae7f-f2b7dda0d35f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2fwT7EPX2YM",
        "outputId": "55b0b3b7-3acf-4bbd-b02e-6886a1ee42bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding unique characters. These are the tokens our model can pull from.\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_WRbOV2YQX1",
        "outputId": "016ab98f-b44b-48fe-ef63-5a1cab2bed0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have finished exploring our dataset. The next step is to figure out how we can tokenize our strings. In English, this is simply the act of encoding our raw string input into integers."
      ],
      "metadata": {
        "id": "2-8slbAfZEQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stringtoint = {ch:i for i,ch in enumerate(chars)}\n",
        "inttostring = {i:ch for i,ch in enumerate(stringtoint)}\n",
        "encode = lambda s: [stringtoint[i] for i in s] #given a string encode and output a sequence of integers\n",
        "decode = lambda i: ''.join([inttostring[j] for j in i])\n",
        "\n",
        "print(encode(\"Hi!\"))\n",
        "print(decode(encode(\"Hi!\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkbM9LSUYvkb",
        "outputId": "4a6bf50a-7795-482d-d0d8-3637197d0271"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 47, 2]\n",
            "Hi!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding entire dataset\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape)\n",
        "print(data[:100]) #what the first 100 characters look like to our transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8YxuML6aHyq",
        "outputId": "8c30db37-cdd5-4138-c5b9-c3e7d40ef56e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data)) #setting 90% as training data and the remaining 10% as validation data\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "block_size = 8 #we train random chunks of the training data to be computationally efficient and these have a length of 8 (also known as context)"
      ],
      "metadata": {
        "id": "C_a0zBf2bDEL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UryG1zPyb8WL",
        "outputId": "b3ca9808-01a0-4964-c5aa-901431cb15af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why +1? Because when training we are actually also looking at the nested relationships between tokens. What does this mean? Well, notice how we have 8 examples in this span of 9 tokens. After 18, 47 is likely to come. After 18 and 47, 56 is likely to come, etc. Therefore, we add the +1 so the model can get 8 examples per block. From an intuitive standpoint this makes sense, as we want our transformer to be able to geenrate predictions from as little as 1 token of input up to block_size and everything in between. "
      ],
      "metadata": {
        "id": "gjTWAqz2cVtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) #In this case batch size is 4, so we are basically randomly grabbing 4 chunks of length 8 from the dataset. These chunks are all independent of each other and mini-batching is used for better compute.\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) #This indexes into data using the ix's generated in the last step and stacks them, constructing the mini-batch\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xj_61Zzb_wY",
        "outputId": "8e49c82b-ca81-4bfd-e9e8-0f6967a12535"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's pass xb and yb into a simple neural network to see what happens. Note: The BigramLanguageModel class is essentially identical to the one previously implemented, just using PyTorch's built in classes for simplicity. "
      ],
      "metadata": {
        "id": "xaByM-xBf7Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #just like the embedding layer we implemented for MLP and WaveNet\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # arranges into a 3d tensor with batch (4) x time (8) x channels (65). Remember logits are nothing but scores (for bigram its counts) of a token following another token.\n",
        "\n",
        "        if targets == None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C) #need to reshape our logits to fit with PyTorch's expectations of F.cross_entropy\n",
        "          targets = targets.view(B*T)\n",
        "          loss = F.cross_entropy(logits, targets) #negative log likelihood\n",
        "        return logits, loss\n",
        "    \n",
        "    #sampling from model\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIrT5G5Wdvw5",
        "outputId": "a3b39216-2bff-4404-e482-a1b6de4daf32"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating optimizer; Adding some sense of optimization to our bigram model\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "DWAXnOV3gJpB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training/optimization loop\n",
        "batch_size = 32\n",
        "\n",
        "for i in range(10000):\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmU4gpItj8E3",
        "outputId": "5ce83afb-6fb8-4b5b-9801-36960d7b074a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.382369041442871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFOpQG4lkh5I",
        "outputId": "71da3062-2f8c-4049-c6e7-70e9139a7462"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "He tha$\n",
            "\n",
            "pinis ty.\n",
            "ABELOLIt,\n",
            "\n",
            "\n",
            "INowniss:\n",
            "This bratheds ar,\n",
            "Whe, wncthatis, ILe; toulis,\n",
            "RDYCHanend\n",
            "O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above text is the output generated from the most basic bigram model. Not bad. But it's also gibberish."
      ],
      "metadata": {
        "id": "tqOLpNYWl0-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve our outputs, we need to add a better notion of context, where each token has some idea of its relationship with its preceding tokens but not any tokens after it (because they are future tokens). We can store this memory as a feature vector and use it to improve the quality of our output. How do we do this? Enter Self-Attention. The trick behind self-attention lies in matrix multiplication and lower triangular matrices. Multiplying our x by these lower triangular matrices allows us to get an idea of how much previous token \"context\" is used (usually average)."
      ],
      "metadata": {
        "id": "WtBTqF2hnPsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self Attention allows information from the past to \"flow\" as context to the current token in a data dependent manner. ie If I'm a vowel, I might want to know what consonants were before me and factor that into my next token prediction. This done in a very clever way, where every token/node emits 2 vectors.\n",
        "\n",
        "\n",
        "1.   Query vector: represents what I want/am looking for\n",
        "2.   Key vector: Represents what I have/contain\n",
        "\n",
        "\"Affinities\" between tokens know are when we take the dot product of the query and key vector for every token. To be more concrete say I'm at token X. I take the dot product between my query at token X and every other key for all other tokens. Remember that the dot product is an abstraction that tells us \"how close\" two vectors are. \n"
      ],
      "metadata": {
        "id": "hclL6cXybmpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "#Looking at how a single head does self-attention\n",
        "#weigh represents the affinities between tokens\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "weigh = q @ k.transpose(-2, -1) #(B, T, 16) @ (B, 16, T) --> (B, T, T) ie for every batch we have a square matrix that captures the affinities between tokens\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "weigh = weigh.masked_fill(tril == 0, float('-inf')) #prevents current token from aggregating information from tokens in front of it (future tokens). That's why the lower triangular matrix is important.\n",
        "weigh = F.softmax(weigh, dim=-1) #normalization operation. Remember that softmax exponentiates and then divides by sum. Rows will end up with a nice distribution that encapsulates the affinities between tokens. \n",
        "\n",
        "v = value(x) #vector that is aggregated rather than the raw x (evident by the headsize of 16)\n",
        "out = weigh @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDYofcApk28l",
        "outputId": "19eb3f9d-044a-4f2e-8449-74a00f5644e0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril #we mask out the weights matrix with 0 and create a lower triangular matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIhjG3EmaslO",
        "outputId": "b677b435-3c9c-434f-b534-10044ca98999"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-softmax weigh and after masking out future tokens. The values are the raw affinities but we normalize them after via softmax to create a nice distribution of tokens we can pick from.\n",
        "weigh[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEZqgZhJfFvM",
        "outputId": "d661c583-aa13-45b5-b080-bb84ae74379f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
              "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
              "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
              "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weigh #we take softmax on every row, where each element now becomes a proportional \"part\" of context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqpG2pGKarWg",
        "outputId": "dd258b7a-0f66-4231-e027-515dfaf2ce97"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to view self-attention is as follows. Every token is a node in a connected directed graph. Each node has some aggregated information (v) that is achieved when we take the weighted sum of all nodes pointing to it with data dependent weights. Note: we should divide our weigh by the sqrt of the head_size becuase this will prevent the weigh values from being too peaky. We want them to be diffuse that way when we enter the softmax, we aren't just converging to a one-hot vector around the max element in that vector, which would just mean each token is getting information from only 1 other token, which is bad. "
      ],
      "metadata": {
        "id": "JTrjNe-wgXQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#single Head module\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x): \n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "jUo3Q26Dat04"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also implement multi-head attention, which is exactly what it sounds like. It's simply multiple heads of attention running parallely and then we concatenate their result. "
      ],
      "metadata": {
        "id": "2iDmaEV8jd-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "li0Vnyxcbc-l"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we add multiple heads, we get improved loss for our bigram model. But after this, from an architectural standpoint, we are basically done. Our bottleneck now lies in the optimization of our model. Specifically, if we add residual connections and layernorms, our optimization process will go much more smoothly and give us better results."
      ],
      "metadata": {
        "id": "FLpBzIZPkD-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next improvement is adding skip/residual connections. These connections can be added very easily to our block module (remember the entire block module is repeated n times in training). The idea behind these residual connections is that when we are doing the backward pass, we have a clear, unimpeded line of gradients from the output backwards to the input. Recall that addition distributes gradients evenly so by adding our input back to the output of some computation, we now have a superhighway of gradients (at least at initialization). As the optimization proceeds, the residual computational forks end up contributing more and more. "
      ],
      "metadata": {
        "id": "7vMTN8UZnz-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next improvement is adding layernorm, which will help with optimization. After adding the residual connections, we have a little bit of overfitting, and our loss has decreased. However, we can further improve the quality of our deep network by adding in layer norm. The only difference w.r.t batch norm is that we normalize the rows of our input rather than the columns. Note: In the paper, layernorms are applied after the transformation but its common practice to do it before instead. "
      ],
      "metadata": {
        "id": "5ed7mn3eoB0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "Rp0Oxtx2kDp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully consolidated code. We added dropout, decreased the learning rate, and increased the batch size. All of these significantly increased the capabilities of the model, as demonstrated by the output."
      ],
      "metadata": {
        "id": "RHzelO3HxGQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--FZR21UkBke",
        "outputId": "3096e11c-ff0f-446f-b6c1-9ec4332a4d57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5059\n",
            "step 300: train loss 2.4196, val loss 2.4335\n",
            "step 400: train loss 2.3503, val loss 2.3565\n",
            "step 500: train loss 2.2967, val loss 2.3132\n",
            "step 600: train loss 2.2411, val loss 2.2502\n",
            "step 700: train loss 2.2064, val loss 2.2198\n",
            "step 800: train loss 2.1640, val loss 2.1869\n",
            "step 900: train loss 2.1243, val loss 2.1508\n",
            "step 1000: train loss 2.1043, val loss 2.1310\n",
            "step 1100: train loss 2.0689, val loss 2.1175\n",
            "step 1200: train loss 2.0379, val loss 2.0791\n",
            "step 1300: train loss 2.0243, val loss 2.0638\n",
            "step 1400: train loss 1.9933, val loss 2.0365\n",
            "step 1500: train loss 1.9700, val loss 2.0298\n",
            "step 1600: train loss 1.9622, val loss 2.0463\n",
            "step 1700: train loss 1.9410, val loss 2.0143\n",
            "step 1800: train loss 1.9082, val loss 1.9951\n",
            "step 1900: train loss 1.9115, val loss 1.9890\n",
            "step 2000: train loss 1.8848, val loss 1.9934\n",
            "step 2100: train loss 1.8711, val loss 1.9740\n",
            "step 2200: train loss 1.8615, val loss 1.9622\n",
            "step 2300: train loss 1.8555, val loss 1.9521\n",
            "step 2400: train loss 1.8428, val loss 1.9446\n",
            "step 2500: train loss 1.8147, val loss 1.9412\n",
            "step 2600: train loss 1.8252, val loss 1.9374\n",
            "step 2700: train loss 1.8141, val loss 1.9339\n",
            "step 2800: train loss 1.8023, val loss 1.9225\n",
            "step 2900: train loss 1.8043, val loss 1.9296\n",
            "step 3000: train loss 1.7965, val loss 1.9182\n",
            "step 3100: train loss 1.7679, val loss 1.9173\n",
            "step 3200: train loss 1.7537, val loss 1.9111\n",
            "step 3300: train loss 1.7576, val loss 1.9040\n",
            "step 3400: train loss 1.7558, val loss 1.8971\n",
            "step 3500: train loss 1.7403, val loss 1.8956\n",
            "step 3600: train loss 1.7253, val loss 1.8861\n",
            "step 3700: train loss 1.7281, val loss 1.8801\n",
            "step 3800: train loss 1.7192, val loss 1.8922\n",
            "step 3900: train loss 1.7230, val loss 1.8734\n",
            "step 4000: train loss 1.7138, val loss 1.8575\n",
            "step 4100: train loss 1.7120, val loss 1.8734\n",
            "step 4200: train loss 1.7104, val loss 1.8673\n",
            "step 4300: train loss 1.7017, val loss 1.8500\n",
            "step 4400: train loss 1.7052, val loss 1.8647\n",
            "step 4500: train loss 1.6910, val loss 1.8535\n",
            "step 4600: train loss 1.6887, val loss 1.8376\n",
            "step 4700: train loss 1.6850, val loss 1.8423\n",
            "step 4800: train loss 1.6677, val loss 1.8443\n",
            "step 4900: train loss 1.6697, val loss 1.8391\n",
            "step 4999: train loss 1.6650, val loss 1.8227\n",
            "\n",
            "And they bride will to loves that seek obe to take OF the called\n",
            "My ard that usque, God?\n",
            "\n",
            "MEXENES:\n",
            "Butwell my feans, wom on me\n",
            "Yourselfofficion of my would but\n",
            "With ensent, will is the ov the does me now our wantes like die; let help on him speak; ands him you love.\n",
            "In Bodiet, and whom.\n",
            "\n",
            "CATINIUS:\n",
            "It onWinsund, their as to them, rivished, in his soun\n",
            "As forgunes and thrust for treagre!\n",
            "But that I let at to figRINCE:\n",
            "So my fries burrtied is:\n",
            "Sadand these help him best as ranny Ire tome\n",
            "from the wound to Pome, griting 'Would that\n",
            "bear hissessenn; fortwer'd madand thou such seen thee;\n",
            "Indend my conforty that that lord\n",
            "In as detture as thouged Peepereing to did the\n",
            "becons if meety some so upon sure fear tals:\n",
            "Whith loves, strike, is duke in,\n",
            "To may I want beakes again, but with some.\n",
            "\n",
            "CORIOLANUS:\n",
            "What last,\n",
            "Our if wich advisure?\n",
            "Their him. Come in now this mine I dood,\n",
            "Aufort or factee that I appriant. what that the must perdless keep you hum\n",
            "For for my wrongetive wout in fort;\n",
            "To know our her and bruned. Plainter wonsursh. wher the virtue that woad young:\n",
            "Way's bring well be weep mines so me worting.\n",
            "\n",
            "GRUFORIZET:\n",
            "You as then, chear with and bewise.Inksmy all the call as ince any.\n",
            "In to doth onatouring feeeir, brothn they have eath I die;\n",
            "Save thou for Hewile too lustlens.\n",
            "\n",
            "MAMILLUS:\n",
            "If the batte I waster uptrunger;\n",
            "Than scrown to his; dresslessy dong'd the not. Frave been smy of vonby the town,\n",
            "Whave baste his make him.\n",
            "\n",
            "Ruman:\n",
            "I will teny, delelege, inso, I have there wollst eneming; sit\n",
            "Inces; and plock, say, as;\n",
            "River those virtuce my sun.\n",
            "\n",
            "PRAANNIUS:\n",
            "Where\n",
            "Oursist arm gentle, drieven himselfry.\n",
            "\n",
            "BENVOLIO:\n",
            "To not desenolving nume. That arm as is infetter,\n",
            "Thhirding as is Prestint the have blay not,\n",
            "That be flantly fint that I races cold, of mundly\n",
            "would thee weak thee with fair? But I have but the peepose: life?\n",
            "\n",
            "COMINIUS:\n",
            "You hove cause; 'tis good not your your son!\n",
            "Nor amonse than a battes\n",
            "Edwill on the Riparding: In if shown'd fivent me suppeen, while wongrave wi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBZbJ_NBxw2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}